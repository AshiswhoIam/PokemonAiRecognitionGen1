Keeping track of what each Results are properly

1)

Batch Size: 64, Learning Rate: 0.00009, L2 Regularization: 0.0005, Dropout Rate: 0.6, Epochs: 60, Layers 5  and 3 dense, 5x5kernel for first then 3x3
flatten
(Very slow training)

- loss: 0.8852 - accuracy: 0.7886 - val_loss: 1.5368 - val_accuracy: 0.6706

Test accuracy: 65.17%
Test Loss: 1.5684

Stopped at 54/60 epochs 

2)

Batch Size: 64, Learning Rate: 0.0002, L2 Regularization: 0.0005, Dropout Rate: 0.5, Epochs: 50, Layers 5  and 3 dense, 5x5kernel for first then 3x3
flatten
- loss: 0.2826 - accuracy: 0.9725 - val_loss: 1.5136 - val_accuracy: 0.6976

Test accuracy: 68.61%
Test Loss: 1.5237

Stopped at 38/50 epochs 

3)

Batch Size: 64, Learning Rate: 0.0003, L2 Regularization: 0.001, Dropout Rate: 0.4, Epochs: 50, Layers 5  and 3 dense, 5x5kernel for first then 3x3
Changing the 512 to 256 and using global instead of flatten

 - loss: 0.8752 - accuracy: 0.8201 - val_loss: 1.6896 - val_accuracy: 0.6450

Test accuracy: 63.18%
Test Loss: 1.7536

50/50

4)

Batch Size: 64, Learning Rate: 0.0001, L2 Regularization: 0.0001, Dropout Rate: 0.3, Epochs: 50, Layers 5  and 3 dense, 5x5kernel for first then 3x3
Removed 768 dense layers,global instead of flatten

Cancelled this process was already way too overfitting early memorizing the data

5)

Batch Size: 64, Learning Rate: 0.0001, L2 Regularization: 0.0001, Dropout Rate: 0.3, Epochs: 55, Layers 4  and 3 dense, 3x3 kernel for first then 3x3
,global instead of flatten


- 57s 340ms/step - loss: 1.2943 - accuracy: 0.7113 - val_loss: 1.9187 - val_accuracy: 0.5863

Test accuracy: 56.47%
Test Loss: 1.9823

6)
Batch Size: 64, Learning Rate: 0.0005, L2 Regularization: 0.001, Dropout Rate: 0.5, Epochs: 50, Layers 6 and 3 dense, 5x5kernel for first then 3x3
flatten

- loss: 0.5953 - accuracy: 0.8909 - val_loss: 1.8616 - val_accuracy: 0.6132

Test accuracy: 59.25%
Test Loss: 1.8766


7)
Using augmented data this time more images 30000~
Batch Size: 64, Learning Rate: 0.0005, L2 Regularization: 0.001, Dropout Rate: 0.4, Epochs: 50, Layers 5(64,128,256,512,512) and 2 dense(1024 512), 3x3kernel for first then 3x3
flatten

- loss: 0.3125 - accuracy: 0.9706 - val_loss: 1.8532 - val_accuracy: 0.6669

Test accuracy: 66.68%
Test Loss: 1.7856

8)
Using augmented data this time more images 30000~
128 batch -> size 5x5

- 117s 689ms/step - loss: 0.2531 - accuracy: 0.9856 - val_loss: 1.7003 - val_accuracy: 0.7013
Test accuracy: 69.59%
Test Loss: 1.6863

33/50

9)
Using augmented data this time more images 30000~
Batch Size: 64, Learning Rate: 0.0002, L2 Regularization: 0.001, Dropout Rate: 0.5, Epochs: 60, Layers 5(64,128,256,512,512) and 2 dense(1024 512), 3x3kernel for first then 3x3
flatten

- 104s 311ms/step - loss: 0.3024 - accuracy: 0.9713 - val_loss: 1.4470 - val_accuracy: 0.7238
Test accuracy: 72.64%
Test Loss: 1.4109

10)
Using augmented data this time more images 30000~
Batch Size: 64, Learning Rate: 0.0001, L2 Regularization: 0.001, Dropout Rate: 0.6, Epochs: 70, Layers 4(64,128,256,x,512) and 2 dense(512 512), 3x3kernel for first then 3x3
flatten


 - loss: 0.4166 - accuracy: 0.8793 - val_loss: 1.4644 - val_accuracy: 0.6574
Test accuracy: 65.07%
Test Loss: 1.4937


 11) This is model 9 with adjustments
Using augmented data this time more images 30000~
Batch Size: 64, Learning Rate: 0.0001, L2 Regularization: 0.001, Dropout Rate: 0.5, Epochs: 80, Layers 5(64,128,256,512,512) and 2 dense(1024 512), 3x3kernel for first then 3x3
flatten added l2 regu on last cv layers 256 512 512 l2 of 0.0005 patience is 4.
- 105s 316ms/step - loss: 0.6507 - accuracy: 0.9898 - val_loss: 1.8118 - val_accuracy: 0.7256
Test accuracy: 73.08%
Test Loss: 1.8220
12)
Using augmented data this time more images 30000~
Batch Size: 64, Learning Rate: 0.001, L2 Regularization: 0.001, Dropout Rate: 0.5, and drop on cv 0.3, Epochs: 80, Layers 5(32,64,128,256,512) and 2 dense(1024 512), 3x3kernel for first then 3x3
flatten added l2 regu on last cv layers 256 512 512 l2 of 0.0005 patience is 4.
- loss: 0.9597 - accuracy: 0.8074 - val_loss: 1.7627 - val_accuracy: 0.6519
Test accuracy: 65.56%
Test Loss: 1.7404

13)Redid 11 with faster learning rate and different augmented data set.
- loss: 0.5758 - accuracy: 0.9132 - val_loss: 1.4684 - val_accuracy: 0.7550
Test accuracy: 73.10%
Test Loss: 1.4246

14)same 11 model
Using weights and changing l2 to 0.0008
Test accuracy: 70.62%
Test Loss: 1.5227

15)
Added proper Weight classes with cap and reverted l2 and added l2 to 2nd cv layer
- loss: 0.3342 - accuracy: 0.9489 - val_loss: 1.2016 - val_accuracy: 0.7606
Test accuracy: 75.74%
Test Loss: 1.1877

Much slower training at the start still overfitting, classification is still worse than my best model

16)(current best) 3/9/2025
addding a 256 layer 
Seems overfitting went up doing this 
The overall classfication is better
- loss: 0.3703 - accuracy: 0.9979 - val_loss: 1.4757 - val_accuracy: 0.7805
Test accuracy: 77.30%
Test Loss: 1.4955
Stopped at epoch 71
17)
Same as 16 just changed lowered the dense layers, and lower lr a bit seems about the same results
classification is 
- loss: 0.5174 - accuracy: 0.9940 - val_loss: 1.7406 - val_accuracy: 0.7785
Test accuracy: 76.51%
Test Loss: 1.7241

Classification was slightly better than 16 by reducing dense layers will keep in mind not sure if bc lr or dense layer.

18)
0.6dropout changes to see seems to be about the same again as well.
classification is 
forgot to track loss and acc about the same tho
Test accuracy: 76.80%
Test Loss: 1.5628

Classification was worse than 17


19)
using batch norm before active relu following model 16
Didnt seem to do much. will check classfication -HORRIBLE
- loss: 0.5223 - accuracy: 0.9797 - val_loss: 1.5177 - val_accuracy: 0.7798
Test accuracy: 75.91%
Test Loss: 1.5187

20)
replaced 256 with a 512 and lowered learn rate a bit
this resulted in higher val acc
As for classfication it was slightly overall better. but it did seem to differ in terms of classes ups and downs b/w diff pokemons

- loss: 0.2222 - accuracy: 0.9991 - val_loss: 1.1790 - val_accuracy: 0.8113
epoc76
Test accuracy: 80.24%
Test Loss: 1.1707
21)
kernel changes, 5x5 on 128 layer
Seems about the same acc,
as for classficiation ~ slightly worse for precision ,f1
- loss: 0.2424 - accuracy: 0.9997 - val_loss: 1.1403 - val_accuracy: 0.8132
Test accuracy: 79.98%
Test Loss: 1.1403
22)
128 256 512x4 the minimal upgrades take a while the later epochs
Accuracy dropped a bit, The classification less than model 20 about same tho
- accuracy: 0.9993 - val_loss: 1.2589 - val_accuracy: 0.7970
Test accuracy: 78.33%
Test Loss: 1.2713

takes too long 100thepoch
23)
100 epoch might be a lot maybe i try 70~next time
For model 23 same hyperparameters as model 20 with lower lr and cv etc same + new data set of augmented images 45k~ this time.

Seems the accuracy went up model is still overfitting, as for classfication

- loss: 0.0974 - accuracy: 0.9999 - val_loss: 0.5553 - val_accuracy: 0.8951
Test accuracy: 90.16%
Test Loss: 0.5160
epoch at 100/100 could go more
