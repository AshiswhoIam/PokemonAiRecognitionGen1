Keeping track of what each Results are properly

1)

Batch Size: 64, Learning Rate: 0.00009, L2 Regularization: 0.0005, Dropout Rate: 0.6, Epochs: 60, Layers 5  and 3 dense, 5x5kernel for first then 3x3
flatten
(Very slow training)

- loss: 0.8852 - accuracy: 0.7886 - val_loss: 1.5368 - val_accuracy: 0.6706

Test accuracy: 65.17%
Test Loss: 1.5684

Stopped at 54/60 epochs 

2)

Batch Size: 64, Learning Rate: 0.0002, L2 Regularization: 0.0005, Dropout Rate: 0.5, Epochs: 50, Layers 5  and 3 dense, 5x5kernel for first then 3x3
flatten
- loss: 0.2826 - accuracy: 0.9725 - val_loss: 1.5136 - val_accuracy: 0.6976

Test accuracy: 68.61%
Test Loss: 1.5237

Stopped at 38/50 epochs 

3)

Batch Size: 64, Learning Rate: 0.0003, L2 Regularization: 0.001, Dropout Rate: 0.4, Epochs: 50, Layers 5  and 3 dense, 5x5kernel for first then 3x3
Changing the 512 to 256 and using global instead of flatten

 - loss: 0.8752 - accuracy: 0.8201 - val_loss: 1.6896 - val_accuracy: 0.6450

Test accuracy: 63.18%
Test Loss: 1.7536

50/50

4)

Batch Size: 64, Learning Rate: 0.0001, L2 Regularization: 0.0001, Dropout Rate: 0.3, Epochs: 50, Layers 5  and 3 dense, 5x5kernel for first then 3x3
Removed 768 dense layers,global instead of flatten

Cancelled this process was already way too overfitting early memorizing the data

5)

Batch Size: 64, Learning Rate: 0.0001, L2 Regularization: 0.0001, Dropout Rate: 0.3, Epochs: 55, Layers 4  and 3 dense, 3x3 kernel for first then 3x3
,global instead of flatten


- 57s 340ms/step - loss: 1.2943 - accuracy: 0.7113 - val_loss: 1.9187 - val_accuracy: 0.5863

Test accuracy: 56.47%
Test Loss: 1.9823

6)
Batch Size: 64, Learning Rate: 0.0005, L2 Regularization: 0.001, Dropout Rate: 0.5, Epochs: 50, Layers 6 and 3 dense, 5x5kernel for first then 3x3
flatten

- loss: 0.5953 - accuracy: 0.8909 - val_loss: 1.8616 - val_accuracy: 0.6132

Test accuracy: 59.25%
Test Loss: 1.8766


7)
Using augmented data this time more images 30000~
Batch Size: 64, Learning Rate: 0.0005, L2 Regularization: 0.001, Dropout Rate: 0.4, Epochs: 50, Layers 5(64,128,256,512,512) and 2 dense(1024 512), 3x3kernel for first then 3x3
flatten

- loss: 0.3125 - accuracy: 0.9706 - val_loss: 1.8532 - val_accuracy: 0.6669

Test accuracy: 66.68%
Test Loss: 1.7856

8)
Using augmented data this time more images 30000~
128 batch -> size 5x5

- 117s 689ms/step - loss: 0.2531 - accuracy: 0.9856 - val_loss: 1.7003 - val_accuracy: 0.7013
Test accuracy: 69.59%
Test Loss: 1.6863

33/50

9)
Using augmented data this time more images 30000~
Batch Size: 64, Learning Rate: 0.0002, L2 Regularization: 0.001, Dropout Rate: 0.5, Epochs: 60, Layers 5(64,128,256,512,512) and 2 dense(1024 512), 3x3kernel for first then 3x3
flatten

- 104s 311ms/step - loss: 0.3024 - accuracy: 0.9713 - val_loss: 1.4470 - val_accuracy: 0.7238
Test accuracy: 72.64%
Test Loss: 1.4109

10)
Using augmented data this time more images 30000~
Batch Size: 64, Learning Rate: 0.0001, L2 Regularization: 0.001, Dropout Rate: 0.6, Epochs: 70, Layers 4(64,128,256,x,512) and 2 dense(512 512), 3x3kernel for first then 3x3
flatten


 - loss: 0.4166 - accuracy: 0.8793 - val_loss: 1.4644 - val_accuracy: 0.6574
Test accuracy: 65.07%
Test Loss: 1.4937


 11)
